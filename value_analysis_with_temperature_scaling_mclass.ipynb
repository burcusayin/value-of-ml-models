{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "value_analysis_with_temperature_scaling_mclass_github.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeTUOfSQSHPD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from collections import namedtuple, Counter, defaultdict\n",
        "import statistics\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import brier_score_loss\n",
        "from scipy.special import softmax\n",
        "from scipy.optimize import minimize \n",
        "import time\n",
        "from keras.losses import categorical_crossentropy\n",
        "from os.path import join\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import log_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This notebook is created to run value analysis with your own model on multi-class datasets\n",
        "#Use this code to first calibrate your model via temperature scaling, and then obtain the value analysis\n",
        "#specify the required info here and run the notebook to receive value analysis result for your model\n",
        "modelName = 'name_of_your_model'\n",
        "resPath = 'define_the_path_for_results'\n",
        "data_folder = 'define_the_path_where_you_keep_the_confidence_values_of_your_model_and_the_datasets'\n",
        "confidencesToVal = 'name_of_the_numpy_array_for_the_confidences_on_validation_set.npy'\n",
        "dataToVal = 'name_of_validation_set.csv'\n",
        "confidencesToTest = 'name_of_the_numpy_array_for_the_confidences_on_test_set.npy'\n",
        "dataToTest ='name_of_test_set.csv'\n",
        "ground_truth_column = 'specify_the_column_for_ground_truth_in_your_csv_files'\n",
        "txt = 'specify_the_column_for_text_in_your_csv_files'\n",
        "datasetName = 'name_of_your_dataset'"
      ],
      "metadata": {
        "id": "LRR7zEtDa2y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlHi5AI8SKAx"
      },
      "source": [
        "#result file\n",
        "logfile_name = \"{}_{}_tscaled\".format(datasetName)\n",
        "\n",
        "#cost-based parameters\n",
        "Vr = 0.0\n",
        "Vc = 1.0\n",
        "Vw_g = list(np.arange(0, -10.1, -1))\n",
        "t_g = list(np.arange(0, 1.01, 0.01))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_val = np.load(data_folder  + confidencesToVal)\n",
        "y_val_df = pd.read_csv(data_folder + dataToVal)\n",
        "y_val = y_val_df[ground_truth_column].values\n",
        "\n",
        "logits_test = np.load(data_folder  + confidencesToTest)\n",
        "y_test_df = pd.read_csv(data_folder + dataToTest)\n",
        "y_test = y_test_df[ground_truth_column].values"
      ],
      "metadata": {
        "id": "IjrUV0I5ZQSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute softmax values for each sets of scores in x.\n",
        "    \n",
        "    Parameters:\n",
        "        x (numpy.ndarray): array containing m samples with n-dimensions (m,n)\n",
        "    Returns:\n",
        "        x_softmax (numpy.ndarray) softmaxed values for initial (m,n) array\n",
        "    \"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=1, keepdims=1)\n",
        "\n",
        "def compute_acc_bin(conf_thresh_lower, conf_thresh_upper, conf, pred, true):\n",
        "    \"\"\"\n",
        "    # Computes accuracy and average confidence for bin\n",
        "    \n",
        "    Args:\n",
        "        conf_thresh_lower (float): Lower Threshold of confidence interval\n",
        "        conf_thresh_upper (float): Upper Threshold of confidence interval\n",
        "        conf (numpy.ndarray): list of confidences\n",
        "        pred (numpy.ndarray): list of predictions\n",
        "        true (numpy.ndarray): list of true labels\n",
        "    \n",
        "    Returns:\n",
        "        (accuracy, avg_conf, len_bin): accuracy of bin, confidence of bin and number of elements in bin.\n",
        "    \"\"\"\n",
        "    filtered_tuples = [x for x in zip(pred, true, conf) if x[2] > conf_thresh_lower and x[2] <= conf_thresh_upper]\n",
        "    if len(filtered_tuples) < 1:\n",
        "        return 0,0,0\n",
        "    else:\n",
        "        correct = len([x for x in filtered_tuples if x[0] == x[1]])  # How many correct labels\n",
        "        len_bin = len(filtered_tuples)  # How many elements falls into given bin\n",
        "        avg_conf = sum([x[2] for x in filtered_tuples]) / len_bin  # Avg confidence of BIN\n",
        "        accuracy = float(correct)/len_bin  # accuracy of BIN\n",
        "        return accuracy, avg_conf, len_bin\n",
        "  \n",
        "\n",
        "def ECE(conf, pred, true, bin_size = 0.1):\n",
        "\n",
        "    \"\"\"\n",
        "    Expected Calibration Error\n",
        "    \n",
        "    Args:\n",
        "        conf (numpy.ndarray): list of confidences\n",
        "        pred (numpy.ndarray): list of predictions\n",
        "        true (numpy.ndarray): list of true labels\n",
        "        bin_size: (float): size of one bin (0,1)  # TODO should convert to number of bins?\n",
        "        \n",
        "    Returns:\n",
        "        ece: expected calibration error\n",
        "    \"\"\"\n",
        "    \n",
        "    upper_bounds = np.arange(bin_size, 1+bin_size, bin_size)  # Get bounds of bins\n",
        "    \n",
        "    n = len(conf)\n",
        "    ece = 0  # Starting error\n",
        "    \n",
        "    for conf_thresh in upper_bounds:  # Go through bounds and find accuracies and confidences\n",
        "        acc, avg_conf, len_bin = compute_acc_bin(conf_thresh-bin_size, conf_thresh, conf, pred, true)        \n",
        "        ece += np.abs(acc-avg_conf)*len_bin/n  # Add weigthed difference to ECE\n",
        "        \n",
        "    return ece\n",
        "        \n",
        "      \n",
        "def MCE(conf, pred, true, bin_size = 0.1):\n",
        "\n",
        "    \"\"\"\n",
        "    Maximal Calibration Error\n",
        "    \n",
        "    Args:\n",
        "        conf (numpy.ndarray): list of confidences\n",
        "        pred (numpy.ndarray): list of predictions\n",
        "        true (numpy.ndarray): list of true labels\n",
        "        bin_size: (float): size of one bin (0,1)  # TODO should convert to number of bins?\n",
        "        \n",
        "    Returns:\n",
        "        mce: maximum calibration error\n",
        "    \"\"\"\n",
        "    \n",
        "    upper_bounds = np.arange(bin_size, 1+bin_size, bin_size)\n",
        "    \n",
        "    cal_errors = []\n",
        "    \n",
        "    for conf_thresh in upper_bounds:\n",
        "        acc, avg_conf, _ = compute_acc_bin(conf_thresh-bin_size, conf_thresh, conf, pred, true)\n",
        "        cal_errors.append(np.abs(acc-avg_conf))\n",
        "        \n",
        "    return max(cal_errors)\n",
        "\n",
        "def evaluate(probs, y_true, verbose = False, normalize = False, bins = 15):\n",
        "    \"\"\"\n",
        "    Evaluate model using various scoring measures: Error Rate, ECE, MCE, NLL\n",
        "    \n",
        "    Params:\n",
        "        probs: a list containing probabilities for all the classes with a shape of (samples, classes)\n",
        "        y_true: a list containing the actual class labels\n",
        "        verbose: (bool) are the scores printed out. (default = False)\n",
        "        normalize: (bool) in case of 1-vs-K calibration, the probabilities need to be normalized.\n",
        "        bins: (int) - into how many bins are probabilities divided (default = 15)\n",
        "        \n",
        "    Returns:\n",
        "        (error, ece, mce, loss), returns various scoring measures\n",
        "    \"\"\"\n",
        "    \n",
        "    preds = np.argmax(probs, axis=1)  # Take maximum confidence as prediction\n",
        "    \n",
        "    if normalize:\n",
        "        confs = np.max(probs, axis=1)/np.sum(probs, axis=1)\n",
        "        # Check if everything below or equal to 1?\n",
        "    else:\n",
        "        confs = np.max(probs, axis=1)  # Take only maximum confidence\n",
        "    \n",
        "    accuracy = metrics.accuracy_score(y_true, preds) * 100\n",
        "    error = 100 - accuracy\n",
        "    \n",
        "        # Calculate ECE\n",
        "    ece = ECE(confs, preds, y_true, bin_size = 1/bins)\n",
        "    # Calculate MCE\n",
        "    mce = MCE(confs, preds, y_true, bin_size = 1/bins)\n",
        "    \n",
        "    loss = log_loss(y_true=y_true, y_pred=probs)\n",
        "    \n",
        "    y_prob_true = np.array([probs[i, idx] for i, idx in enumerate(y_true)])  # Probability of positive class\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Accuracy:\", accuracy)\n",
        "        print(\"Error:\", error)\n",
        "        print(\"ECE:\", ece)\n",
        "        print(\"MCE:\", mce)\n",
        "        print(\"Loss:\", loss)\n",
        "    \n",
        "    return (error, ece, mce, loss)\n",
        "\n",
        "\n",
        "class TemperatureScaling():\n",
        "    \n",
        "    def __init__(self, temp = 1, maxiter = 50, solver = \"BFGS\"):\n",
        "        \"\"\"\n",
        "        Initialize class\n",
        "        \n",
        "        Params:\n",
        "            temp (float): starting temperature, default 1\n",
        "            maxiter (int): maximum iterations done by optimizer, however 8 iterations have been maximum.\n",
        "        \"\"\"\n",
        "        self.temp = temp\n",
        "        self.maxiter = maxiter\n",
        "        self.solver = solver\n",
        "    \n",
        "    def _loss_fun(self, x, probs, true):\n",
        "        # Calculates the loss using log-loss (cross-entropy loss)\n",
        "        scaled_probs = self.predict(probs, x)    \n",
        "        loss = log_loss(y_true=true, y_pred=scaled_probs)\n",
        "        return loss\n",
        "    \n",
        "    # Find the temperature\n",
        "    def fit(self, logits, true):\n",
        "        \"\"\"\n",
        "        Trains the model and finds optimal temperature\n",
        "        \n",
        "        Params:\n",
        "            logits: the output from neural network for each class (shape [samples, classes])\n",
        "            true: one-hot-encoding of true labels.\n",
        "            \n",
        "        Returns:\n",
        "            the results of optimizer after minimizing is finished.\n",
        "        \"\"\"\n",
        "        \n",
        "        true = true.flatten() # Flatten y_val\n",
        "        opt = minimize(self._loss_fun, x0 = 1, args=(logits, true), options={'maxiter':self.maxiter}, method = self.solver)\n",
        "        self.temp = opt.x[0]\n",
        "        \n",
        "        return opt\n",
        "        \n",
        "    def predict(self, logits, temp = None):\n",
        "        \"\"\"\n",
        "        Scales logits based on the temperature and returns calibrated probabilities\n",
        "        \n",
        "        Params:\n",
        "            logits: logits values of data (output from neural network) for each class (shape [samples, classes])\n",
        "            temp: if not set use temperatures find by model or previously set.\n",
        "            \n",
        "        Returns:\n",
        "            calibrated probabilities (nd.array with shape [samples, classes])\n",
        "        \"\"\"\n",
        "        \n",
        "        if not temp:\n",
        "            return softmax(logits/self.temp)\n",
        "        else:\n",
        "            return softmax(logits/temp)\n",
        "\n",
        "class CELoss(object):\n",
        "\n",
        "    def compute_bin_boundaries(self, probabilities = np.array([])):\n",
        "\n",
        "        #uniform bin spacing\n",
        "        if probabilities.size == 0:\n",
        "            bin_boundaries = np.linspace(0, 1, self.n_bins + 1)\n",
        "            self.bin_lowers = bin_boundaries[:-1]\n",
        "            self.bin_uppers = bin_boundaries[1:]\n",
        "        else:\n",
        "            #size of bins \n",
        "            bin_n = int(self.n_data/self.n_bins)\n",
        "\n",
        "            bin_boundaries = np.array([])\n",
        "\n",
        "            probabilities_sort = np.sort(probabilities)  \n",
        "\n",
        "            for i in range(0,self.n_bins):\n",
        "                bin_boundaries = np.append(bin_boundaries,probabilities_sort[i*bin_n])\n",
        "            bin_boundaries = np.append(bin_boundaries,1.0)\n",
        "\n",
        "            self.bin_lowers = bin_boundaries[:-1]\n",
        "            self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "\n",
        "    def get_probabilities(self, output, labels, logits):\n",
        "        #If not probabilities apply softmax!\n",
        "        if logits:\n",
        "            self.probabilities = softmax(output, axis=1)\n",
        "        else:\n",
        "            self.probabilities = output\n",
        "\n",
        "        self.labels = labels\n",
        "        self.confidences = np.max(self.probabilities, axis=1)\n",
        "        self.predictions = np.argmax(self.probabilities, axis=1)\n",
        "        self.accuracies = np.equal(self.predictions,labels)\n",
        "\n",
        "    def binary_matrices(self):\n",
        "        idx = np.arange(self.n_data)\n",
        "        #make matrices of zeros\n",
        "        pred_matrix = np.zeros([self.n_data,self.n_class])\n",
        "        label_matrix = np.zeros([self.n_data,self.n_class])\n",
        "        #self.acc_matrix = np.zeros([self.n_data,self.n_class])\n",
        "        pred_matrix[idx,self.predictions] = 1\n",
        "        label_matrix[idx,self.labels] = 1\n",
        "\n",
        "        self.acc_matrix = np.equal(pred_matrix, label_matrix)\n",
        "\n",
        "\n",
        "    def compute_bins(self, index = None):\n",
        "        self.bin_prop = np.zeros(self.n_bins)\n",
        "        self.bin_acc = np.zeros(self.n_bins)\n",
        "        self.bin_conf = np.zeros(self.n_bins)\n",
        "        self.bin_score = np.zeros(self.n_bins)\n",
        "\n",
        "        if index == None:\n",
        "            confidences = self.confidences\n",
        "            accuracies = self.accuracies\n",
        "        else:\n",
        "            confidences = self.probabilities[:,index]\n",
        "            accuracies = self.acc_matrix[:,index]\n",
        "\n",
        "\n",
        "        for i, (bin_lower, bin_upper) in enumerate(zip(self.bin_lowers, self.bin_uppers)):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = np.greater(confidences,bin_lower.item()) * np.less_equal(confidences,bin_upper.item())\n",
        "            self.bin_prop[i] = np.mean(in_bin)\n",
        "\n",
        "            if self.bin_prop[i].item() > 0:\n",
        "                self.bin_acc[i] = np.mean(accuracies[in_bin])\n",
        "                self.bin_conf[i] = np.mean(confidences[in_bin])\n",
        "                self.bin_score[i] = np.abs(self.bin_conf[i] - self.bin_acc[i])\n",
        "\n",
        "class MaxProbCELoss(CELoss):\n",
        "    def loss(self, output, labels, n_bins = 15, logits = True):\n",
        "        self.n_bins = n_bins\n",
        "        super().compute_bin_boundaries()\n",
        "        super().get_probabilities(output, labels, logits)\n",
        "        super().compute_bins()\n",
        "\n",
        "#http://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf\n",
        "class ECELoss(MaxProbCELoss):\n",
        "\n",
        "    def loss(self, output, labels, n_bins = 15, logits = False):\n",
        "        super().loss(output, labels, n_bins, logits)\n",
        "        return np.dot(self.bin_prop,self.bin_score)\n",
        "\n",
        "class MCELoss(MaxProbCELoss):\n",
        "    \n",
        "    def loss(self, output, labels, n_bins = 15, logits = True):\n",
        "        super().loss(output, labels, n_bins, logits)\n",
        "        return np.max(self.bin_score)\n",
        "\n",
        "#https://arxiv.org/abs/1905.11001\n",
        "#Overconfidence Loss (Good in high risk applications where confident but wrong predictions can be especially harmful)\n",
        "class OELoss(MaxProbCELoss):\n",
        "\n",
        "    def loss(self, output, labels, n_bins = 15, logits = True):\n",
        "        super().loss(output, labels, n_bins, logits)\n",
        "        return np.dot(self.bin_prop,self.bin_conf * np.maximum(self.bin_conf-self.bin_acc,np.zeros(self.n_bins)))\n",
        "\n",
        "\n",
        "#https://arxiv.org/abs/1904.01685\n",
        "class SCELoss(CELoss):\n",
        "\n",
        "    def loss(self, output, labels, n_bins = 15, logits = True):\n",
        "        sce = 0.0\n",
        "        self.n_bins = n_bins\n",
        "        self.n_data = len(output)\n",
        "        self.n_class = len(output[0])\n",
        "\n",
        "        super().compute_bin_boundaries()\n",
        "        super().get_probabilities(output, labels, logits)\n",
        "        super().binary_matrices()\n",
        "\n",
        "        for i in range(self.n_class):\n",
        "            super().compute_bins(i)\n",
        "            sce += np.dot(self.bin_prop,self.bin_score)\n",
        "\n",
        "        return sce/self.n_class\n",
        "\n",
        "class TACELoss(CELoss):\n",
        "\n",
        "    def loss(self, output, labels, threshold = 0.01, n_bins = 15, logits = True):\n",
        "        tace = 0.0\n",
        "        self.n_bins = n_bins\n",
        "        self.n_data = len(output)\n",
        "        self.n_class = len(output[0])\n",
        "\n",
        "        super().get_probabilities(output, labels, logits)\n",
        "        self.probabilities[self.probabilities < threshold] = 0\n",
        "        super().binary_matrices()\n",
        "\n",
        "        for i in range(self.n_class):\n",
        "            super().compute_bin_boundaries(self.probabilities[:,i]) \n",
        "            super().compute_bins(i)\n",
        "            tace += np.dot(self.bin_prop,self.bin_score)\n",
        "\n",
        "        return tace/self.n_class\n",
        "\n",
        "#create TACELoss with threshold fixed at 0\n",
        "class ACELoss(TACELoss):\n",
        "\n",
        "    def loss(self, output, labels, n_bins = 15, logits = False):\n",
        "        return super().loss(output, labels, 0.0 , n_bins, logits)\n",
        "\n",
        "\n",
        "def cal_results(fn, logits_val, y_val, logits_test, y_test, m_kwargs = {}, approach = \"all\"):\n",
        "  \n",
        "    if approach == \"all\":            \n",
        "\n",
        "        y_val = y_val.flatten()\n",
        "\n",
        "        model = fn(**m_kwargs)\n",
        "\n",
        "        model.fit(logits_val, y_val)\n",
        "\n",
        "        probs_val = model.predict(logits_val) \n",
        "        probs_test = model.predict(logits_test)\n",
        "        error, ece, mce, loss = evaluate(probs_test, y_test, verbose=True)\n",
        "            \n",
        "        print(\"Error %f; ece %f; mce %f; loss %f\" % evaluate(probs_val, y_val, verbose=False, normalize=True))\n",
        "\n",
        "            \n",
        "    else:  # 1-vs-k models\n",
        "        probs_val = softmax(logits_val)  # Softmax logits\n",
        "        probs_test = softmax(logits_test)\n",
        "        K = probs_test.shape[1]\n",
        "            \n",
        "        # Go through all the classes\n",
        "        for k in range(K):\n",
        "            # Prep class labels (1 fixed true class, 0 other classes)\n",
        "            y_cal = np.array(y_val == k, dtype=\"int\")[:, 0]\n",
        "\n",
        "            # Train model\n",
        "            model = fn(**m_kwargs)\n",
        "            model.fit(probs_val[:, k], y_cal) # Get only one column with probs for given class \"k\"\n",
        "\n",
        "            probs_val[:, k] = model.predict(probs_val[:, k])  # Predict new values based on the fittting\n",
        "            probs_test[:, k] = model.predict(probs_test[:, k])\n",
        "\n",
        "            # Replace NaN with 0, as it should be close to zero  # TODO is it needed?\n",
        "            idx_nan = np.where(np.isnan(probs_test))\n",
        "            probs_test[idx_nan] = 0\n",
        "\n",
        "            idx_nan = np.where(np.isnan(probs_val))\n",
        "            probs_val[idx_nan] = 0\n",
        "\n",
        "        # Get results for test set\n",
        "        error, ece, mce, loss = evaluate(probs_test, y_test, verbose=True, normalize=True)\n",
        "            \n",
        "        print(\"Error %f; ece %f; mce %f; loss %f\" % evaluate(probs_val, y_val, verbose=False, normalize=True))\n",
        "            \n",
        "        \n",
        "    return probs_val, probs_test"
      ],
      "metadata": {
        "id": "h286u_ILb2ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK1wDvSfSiwx"
      },
      "source": [
        "def cost_based_threshold(k):\n",
        "    t = (k)/(k+1)\n",
        "    return t\n",
        "\n",
        "def calculate_value(y_hat_proba, y, t, Vr, Vc, Vw):\n",
        "\n",
        "    y_pred = np.array([np.where(l == np.amax(l))[0][0] if (np.amax(l) > t) else -1 for l in y_hat_proba])\n",
        "\n",
        "    # now lets compute the actual value of each prediction\n",
        "    \n",
        "    value_vector = np.full(y_pred.shape[0], Vc)\n",
        "\n",
        "    value_vector[(y_pred != y) & (y_pred != -1)] = Vw\n",
        "    \n",
        "    #loss due to asking humans\n",
        "    value_vector[y_pred == -1] = Vr\n",
        "    value = np.sum(value_vector) / len(y)\n",
        "\n",
        "    numOfRejectedSamples = np.count_nonzero(y_pred == -1)\n",
        "    numOfWrongPredictions = np.count_nonzero((y_pred != y) & (y_pred != -1))\n",
        "    return value, numOfRejectedSamples, numOfWrongPredictions\n",
        "\n",
        "def find_optimum_confidence_threshold(y_hat_proba, y, t_list, Vr, Vc, Vw):\n",
        "\n",
        "    cost_list = {}\n",
        "\n",
        "    for t in t_list:\n",
        "        value, _ , __ = calculate_value(y_hat_proba, y, t, Vr, Vc, Vw)\n",
        "        cost_list[\"{}\".format(t)] = value\n",
        "    # find t values with maximum value\n",
        "    maxValue = max(cost_list.values())\n",
        "    optTList = [float(k) for k, v in cost_list.items() if v == maxValue]\n",
        "    # pick the one with the lowest confidence\n",
        "    optimumT = min(optTList)\n",
        "\n",
        "    return optimumT, cost_list \n",
        "\n",
        "#cost based calibration analysis\n",
        "def cost_based_analysis(y_hat_proba_val, y_val, y_hat_proba_test, y_test, res_path, logfile_name, Vr, Vc, Vw_list, confT_list):\n",
        "\n",
        "    # create log file\n",
        "    rc_path = res_path + logfile_name + \"_costBased_test.csv\"\n",
        "    with open(rc_path, 'w') as f:\n",
        "        c = 'Vr, Vc, Vw, k, t, value, rejected, wrong, t_optimal, value_optimal, rejected_opt, wrong_opt'\n",
        "        f.write(c + '\\n')\n",
        "\n",
        "    for Vw in Vw_list:\n",
        "        k = (-1)*(Vw / Vc)\n",
        "        t = cost_based_threshold(k)\n",
        "        value_test, rej_test, wrong_test = calculate_value(y_hat_proba_test, y_test, t, Vr, Vc, Vw)\n",
        "\n",
        "        t_optimal, cost_list = find_optimum_confidence_threshold(y_hat_proba_val, y_val, confT_list, Vr, Vc, Vw)\n",
        "        value_test_opt, rej_test_opt, wrong_test_opt = calculate_value(y_hat_proba_test, y_test, t_optimal, Vr, Vc, Vw)\n",
        "\n",
        "        with open(rc_path, 'a') as f:\n",
        "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(Vr, Vc, Vw, k, t, value_test, rej_test, wrong_test, t_optimal, value_test_opt, rej_test_opt, wrong_test_opt)\n",
        "            f.write(res_i)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_val_cal, logits_test_cal = cal_results(TemperatureScaling, logits_val, y_val, logits_test, y_test, approach = \"1-vs-k\")"
      ],
      "metadata": {
        "id": "1qZQvzkVcBaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1RkiahOSl1v"
      },
      "source": [
        "cost_based_analysis(logits_val, y_val, logits_test, y_test, res_path, logfile_name, Vr, Vc, Vw_g, t_g)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}