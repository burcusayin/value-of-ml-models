{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tuning_and_testing_GPT3_on_MDS_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RED1qIek9ghR",
        "Z1IaaN6SqNx3",
        "ec4n1Yrsqbk4",
        "zLtPWp4Jqg-P",
        "t6S27lWbqlc-",
        "XUknv7nPqpmZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO7PSl0zniZp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install openai\n",
        "from transformers import GPT2TokenizerFast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import openai\n",
        "import time\n",
        "from transformers import GPT2TokenizerFast\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "openai.api_key = \"insert your key here\"\n",
        "source_folder = 'define_the_path_where_you_keep_MDS_Data'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***PREPARING THE DATASET FOR FINE-TUNING GPT-3***"
      ],
      "metadata": {
        "id": "mmNol6gkoFJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_save_data(df, source_folder, target):\n",
        "\n",
        "    prompts = df[\"prompt\"].to_list()\n",
        "    largeStringIndices = []\n",
        "    for i in range(len(prompts)):\n",
        "        tokens = tokenizer.encode(prompts[i] + \" ->\")\n",
        "        if len(tokens)>2049:\n",
        "            largeStringIndices.append(i)\n",
        "\n",
        "    df = df[~df.index.isin(largeStringIndices)]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df = df[['prompt', 'completion']]\n",
        "    df.to_csv(source_folder + 'openai-parsed_{}_val_cleaned.csv'.format(target))"
      ],
      "metadata": {
        "id": "25a07nbapiZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sources = ['dvd', 'books', 'electronics', 'kitchen']\n",
        "\n",
        "for source in sources:\n",
        "    dataToTrain = '{}_train.csv'.format(source)\n",
        "    dataToVal = '{}_val.csv'.format(source)\n",
        "    dataToTest ='{}_test.csv'.format(source)\n",
        "\n",
        "    trainD = pd.read_csv(source_folder + dataToTrain)\n",
        "    valD = pd.read_csv(source_folder + dataToVal)\n",
        "    testD = pd.read_csv(source_folder + dataToTest)\n",
        "\n",
        "    df_train = trainD.rename(columns={txt: 'prompt', ground_truth_column: 'completion'}).reset_index(drop=True)\n",
        "    del df_train[\"Unnamed: 0\"]\n",
        "    df_train['prompt'] = df_train['prompt'].apply(lambda x : x.rjust(len(x) + 1))   \n",
        "    clean_and_save_data(df_train, source_folder, target)\n",
        "\n",
        "    df_val = valD.rename(columns={txt: 'prompt', ground_truth_column: 'completion'}).reset_index(drop=True)\n",
        "    del df_val[\"Unnamed: 0\"]\n",
        "    df_val['prompt'] = df_val['prompt'].apply(lambda x : x.rjust(len(x) + 1))\n",
        "    clean_and_save_data(df_val, source_folder, target)\n",
        "\n",
        "    df_test = testD.rename(columns={txt: 'prompt', ground_truth_column: 'completion'}).reset_index(drop=True)\n",
        "    del df_test[\"Unnamed: 0\"]\n",
        "    df_test['prompt'] = df_test['prompt'].apply(lambda x : x.rjust(len(x) + 1))\n",
        "    clean_and_save_data(df_test, source_folder, target)\n"
      ],
      "metadata": {
        "id": "rwKIGGg5oEQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***FINE-TUNE GPT-3 ON MDS DATASET***"
      ],
      "metadata": {
        "id": "RED1qIek9ghR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "export OPENAI_API_KEY=\"insert your key\"\n",
        "\n",
        "#insert the required info into below script\n",
        "#FINE-TUNE ON DVD\n",
        "openai api fine_tunes.create -t \"path_to_where_you_keep_the_data/openai-parsed_dvd_train_prepared.jsonl\" -v \"path_to_where_you_keep_the_data/openai-parsed_dvd_val_prepared.jsonl\"  --batch_size 64 --compute_classification_metrics --classification_n_classes 2 --classification_positive_class ' 1' -m ada --suffix \"specify the name of your fine-tuned model here\""
      ],
      "metadata": {
        "id": "gEugt-Eb9xYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FINE-TUNE ON Books\n",
        "openai api fine_tunes.create -t \"path_to_where_you_keep_the_data/openai-parsed_books_train_prepared.jsonl\" -v \"path_to_where_you_keep_the_data/openai-parsed_books_val_prepared.jsonl\"  --batch_size 64 --compute_classification_metrics --classification_n_classes 2 --classification_positive_class ' 1' -m ada --suffix \"specify the name of your fine-tuned model here\""
      ],
      "metadata": {
        "id": "azjj3_fP_yDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FINE-TUNE ON Electronics\n",
        "openai api fine_tunes.create -t \"path_to_where_you_keep_the_data/openai-parsed_electronics_train_prepared.jsonl\" -v \"path_to_where_you_keep_the_data/openai-parsed_electronics_val_prepared.jsonl\"  --batch_size 64 --compute_classification_metrics --classification_n_classes 2 --classification_positive_class ' 1' -m ada --suffix \"specify the name of your fine-tuned model here\""
      ],
      "metadata": {
        "id": "A39FFVjJ_yS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FINE-TUNE ON Kitchen\n",
        "openai api fine_tunes.create -t \"path_to_where_you_keep_the_data/openai-parsed_kitchen_train_prepared.jsonl\" -v \"path_to_where_you_keep_the_data/openai-parsed_kitchen_val_prepared.jsonl\"  --batch_size 64 --compute_classification_metrics --classification_n_classes 2 --classification_positive_class ' 1' -m ada --suffix \"specify the name of your fine-tuned model here\""
      ],
      "metadata": {
        "id": "4loDoYlz_yiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to see the fine-tuned models and their info, use the script below\n",
        "openai api fine_tunes.list"
      ],
      "metadata": {
        "id": "ReorH4rE-4eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***TEST THE FINE-TUNED MODELS ON DATASET***"
      ],
      "metadata": {
        "id": "Z1IaaN6SqNx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fineTunedModel_DVD = \"specify the name of your model fine-tuned on DVD domain\"\n",
        "fineTunedModel_Books = \"specify the name of your model fine-tuned on Books domain\"\n",
        "fineTunedModel_Electronics = \"specify the name of your model fine-tuned on Electronics domain\"\n",
        "fineTunedModel_Kitchen = \"specify the name of your model fine-tuned on Kitchen domain\""
      ],
      "metadata": {
        "id": "VhCKXA_0rL3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logprob_to_prob(logprob):\n",
        "    return np.exp(logprob)"
      ],
      "metadata": {
        "id": "YMSsgMShqS3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SOURCE : DVD***"
      ],
      "metadata": {
        "id": "ec4n1Yrsqbk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = 'dvd'\n",
        "targets = ['books', 'electronics', 'kitchen']\n",
        "\n",
        "for target in targets:\n",
        "    target_df = pd.read_csv(source_folder + 'openai-parsed_{}_val_cleaned.csv'.format(target))\n",
        "    prompts = target_df[\"prompt\"].to_list()\n",
        "    probs_test = []\n",
        "    for i in range(len(prompts)):\n",
        "        p = prompts[i]\n",
        "        logits = openai.Completion.create(\n",
        "            model=fineTunedModel_DVD,\n",
        "            prompt=p + \" ->\",\n",
        "            temperature=0,\n",
        "            max_tokens=1,\n",
        "            logprobs =2)\n",
        "        logits_neg = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 0\"]\n",
        "        logits_pos = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 1\"]\n",
        "        prob_neg = logprob_to_prob(logits_neg)\n",
        "        prob_pos = logprob_to_prob(logits_pos)\n",
        "        probs_test.append([prob_neg, prob_pos])\n",
        "        time.sleep(2)\n",
        "\n",
        "    np.save(source_folder + 'multiDomainSentiment_val_GPT3_S-{}_T-{}_tcai.npy'.format(source, target), probs_test)\n",
        "    \n",
        "for target in targets:\n",
        "    target_df = pd.read_csv(source_folder + 'openai-parsed_{}_test_cleaned.csv'.format(target))\n",
        "    prompts = target_df[\"prompt\"].to_list()\n",
        "    probs_test = []\n",
        "    for i in range(len(prompts)):\n",
        "        p = prompts[i]\n",
        "        logits = openai.Completion.create(\n",
        "            model=fineTunedModel_DVD,\n",
        "            prompt=p + \" ->\",\n",
        "            temperature=0,\n",
        "            max_tokens=1,\n",
        "            logprobs =2)\n",
        "        logits_neg = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 0\"]\n",
        "        logits_pos = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 1\"]\n",
        "        prob_neg = logprob_to_prob(logits_neg)\n",
        "        prob_pos = logprob_to_prob(logits_pos)\n",
        "        probs_test.append([prob_neg, prob_pos])\n",
        "        time.sleep(2)\n",
        "\n",
        "    np.save(source_folder + 'multiDomainSentiment_test_GPT3_S-{}_T-{}_tcai.npy'.format(source, target), probs_test)"
      ],
      "metadata": {
        "id": "5hP4vICpqfqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SOURCE : BOOKS***"
      ],
      "metadata": {
        "id": "zLtPWp4Jqg-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = 'books'\n",
        "targets = ['dvd', 'electronics', 'kitchen']\n",
        "for target in targets:\n",
        "    target_df = pd.read_csv(source_folder + 'openai-parsed_{}_val_cleaned.csv'.format(target))\n",
        "    prompts = target_df[\"prompt\"].to_list()\n",
        "    probs_test = []\n",
        "    for i in range(len(prompts)):\n",
        "        p = prompts[i]\n",
        "        logits = openai.Completion.create(\n",
        "            model=fineTunedModel_Books,\n",
        "            prompt=p + \" ->\",\n",
        "            temperature=0,\n",
        "            max_tokens=1,\n",
        "            logprobs =2)\n",
        "        logits_neg = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 0\"]\n",
        "        logits_pos = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 1\"]\n",
        "        prob_neg = logprob_to_prob(logits_neg)\n",
        "        prob_pos = logprob_to_prob(logits_pos)\n",
        "        probs_test.append([prob_neg, prob_pos])\n",
        "        time.sleep(2)\n",
        "    np.save(source_folder + 'multiDomainSentiment_val_GPT3_S-{}_T-{}_tcai.npy'.format(source, target), probs_test)\n",
        "for target in targets:\n",
        "    target_df = pd.read_csv(source_folder + 'openai-parsed_{}_test_cleaned.csv'.format(target))\n",
        "    prompts = target_df[\"prompt\"].to_list()\n",
        "    probs_test = []\n",
        "    for i in range(len(prompts)):\n",
        "        p = prompts[i]\n",
        "        logits = openai.Completion.create(\n",
        "            model=fineTunedModel_Books,\n",
        "            prompt=p + \" ->\",\n",
        "            temperature=0,\n",
        "            max_tokens=1,\n",
        "            logprobs =2)\n",
        "        logits_neg = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 0\"]\n",
        "        logits_pos = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 1\"]\n",
        "        prob_neg = logprob_to_prob(logits_neg)\n",
        "        prob_pos = logprob_to_prob(logits_pos)\n",
        "        probs_test.append([prob_neg, prob_pos])\n",
        "        time.sleep(2)\n",
        "    np.save(source_folder + 'multiDomainSentiment_test_GPT3_S-{}_T-{}_tcai.npy'.format(source, target), probs_test)"
      ],
      "metadata": {
        "id": "l0U-Ol_7qkhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SOURCE : ELECTRONICS***"
      ],
      "metadata": {
        "id": "t6S27lWbqlc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = 'electronics'\n",
        "targets = ['dvd', 'books', 'kitchen']\n",
        "for target in targets:\n",
        "    target_df = pd.read_csv(source_folder + 'openai-parsed_{}_val_cleaned.csv'.format(target))\n",
        "    prompts = target_df[\"prompt\"].to_list()\n",
        "    probs_test = []\n",
        "    for i in range(len(prompts)):\n",
        "        p = prompts[i]\n",
        "        logits = openai.Completion.create(\n",
        "            model=fineTunedModel_Electronics,\n",
        "            prompt=p + \" ->\",\n",
        "            temperature=0,\n",
        "            max_tokens=1,\n",
        "            logprobs =2)\n",
        "        logits_neg = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 0\"]\n",
        "        logits_pos = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 1\"]\n",
        "        prob_neg = logprob_to_prob(logits_neg)\n",
        "        prob_pos = logprob_to_prob(logits_pos)\n",
        "        probs_test.append([prob_neg, prob_pos])\n",
        "        time.sleep(2)\n",
        "    np.save(source_folder + 'multiDomainSentiment_val_GPT3_S-{}_T-{}_tcai.npy'.format(source, target), probs_test)\n",
        "for target in targets:\n",
        "    target_df = pd.read_csv(source_folder + 'openai-parsed_{}_test_cleaned.csv'.format(target))\n",
        "    prompts = target_df[\"prompt\"].to_list()\n",
        "    probs_test = []\n",
        "    for i in range(len(prompts)):\n",
        "        p = prompts[i]\n",
        "        logits = openai.Completion.create(\n",
        "            model=fineTunedModel_Electronics,\n",
        "            prompt=p + \" ->\",\n",
        "            temperature=0,\n",
        "            max_tokens=1,\n",
        "            logprobs =2)\n",
        "        logits_neg = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 0\"]\n",
        "        logits_pos = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 1\"]\n",
        "        prob_neg = logprob_to_prob(logits_neg)\n",
        "        prob_pos = logprob_to_prob(logits_pos)\n",
        "        probs_test.append([prob_neg, prob_pos])\n",
        "        time.sleep(2)\n",
        "    np.save(source_folder + 'multiDomainSentiment_test_GPT3_S-{}_T-{}_tcai.npy'.format(source, target), probs_test)"
      ],
      "metadata": {
        "id": "p59zal_Rqo0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SOURCE : KITCHEN***"
      ],
      "metadata": {
        "id": "XUknv7nPqpmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = 'kitchen'\n",
        "targets = ['dvd','books','electronics']\n",
        "for target in targets:\n",
        "    target_df = pd.read_csv(source_folder + 'openai-parsed_{}_val_cleaned.csv'.format(target))\n",
        "    prompts = target_df[\"prompt\"].to_list()\n",
        "    probs_test = []\n",
        "    for i in range(len(prompts)):\n",
        "        p = prompts[i]\n",
        "        logits = openai.Completion.create(\n",
        "            model=fineTunedModel_Kitchen,\n",
        "            prompt=p + \" ->\",\n",
        "            temperature=0,\n",
        "            max_tokens=1,\n",
        "            logprobs =2)\n",
        "        logits_neg = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 0\"]\n",
        "        logits_pos = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 1\"]\n",
        "        prob_neg = logprob_to_prob(logits_neg)\n",
        "        prob_pos = logprob_to_prob(logits_pos)\n",
        "        probs_test.append([prob_neg, prob_pos])\n",
        "        time.sleep(2)\n",
        "        np.save(source_folder + 'multiDomainSentiment_val_GPT3_S-{}_T-{}_tcai.npy'.format(source, target), probs_test)\n",
        "for target in targets:\n",
        "    target_df = pd.read_csv(source_folder + 'openai-parsed_{}_test_cleaned.csv'.format(target))\n",
        "    prompts = target_df[\"prompt\"].to_list()\n",
        "    probs_test = []\n",
        "    for i in range(len(prompts)):\n",
        "        p = prompts[i]\n",
        "        logits = openai.Completion.create(\n",
        "            model=fineTunedModel_Kitchen,\n",
        "            prompt=p + \" ->\",\n",
        "            temperature=0,\n",
        "            max_tokens=1,\n",
        "            logprobs =2)\n",
        "        logits_neg = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 0\"]\n",
        "        logits_pos = logits[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0][\" 1\"]\n",
        "        prob_neg = logprob_to_prob(logits_neg)\n",
        "        prob_pos = logprob_to_prob(logits_pos)\n",
        "        probs_test.append([prob_neg, prob_pos])\n",
        "        time.sleep(2)\n",
        "        np.save(source_folder + 'multiDomainSentiment_test_GPT3_S-{}_T-{}_tcai.npy'.format(source, target), probs_test)"
      ],
      "metadata": {
        "id": "rVPJjV58qsDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}